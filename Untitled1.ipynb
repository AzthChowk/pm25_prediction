{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8548454d-0c0e-429a-aa4c-f38e304d5cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\thhnp\\thesis pm25 prediction\\tfvenv\\lib\\site-packages (2.3.4)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\thhnp\\thesis pm25 prediction\\tfvenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\thhnp\\thesis pm25 prediction\\tfvenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/8.7 MB 8.1 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.4/8.7 MB 10.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 5.8/8.7 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.1/8.7 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 10.1 MB/s eta 0:00:00\n",
      "Downloading pandas-2.3.3-cp313-cp313-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 1.8/11.0 MB 8.7 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.9/11.0 MB 9.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.8/11.0 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.9/11.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/11.0 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 9.7 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading scipy-1.16.3-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "   ---------------------------------------- 0.0/38.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.1/38.5 MB 10.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 4.5/38.5 MB 10.7 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 6.3/38.5 MB 10.5 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 8.7/38.5 MB 10.6 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 10.7/38.5 MB 10.4 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 12.8/38.5 MB 10.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 15.5/38.5 MB 10.7 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 17.8/38.5 MB 10.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 20.2/38.5 MB 10.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.5/38.5 MB 11.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 25.4/38.5 MB 11.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.0/38.5 MB 11.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.4/38.5 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.0/38.5 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 35.4/38.5 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.0/38.5 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.5/38.5 MB 10.9 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, scipy, joblib, scikit-learn, pandas\n",
      "Successfully installed joblib-1.5.2 pandas-2.3.3 pytz-2025.2 scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\thhnp\\Thesis PM25 Prediction\\tfvenv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee93f8c4-8047-434d-b7cf-a7a94bec4363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.1.1-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\thhnp\\thesis pm25 prediction\\tfvenv\\lib\\site-packages (from xgboost) (2.3.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\thhnp\\thesis pm25 prediction\\tfvenv\\lib\\site-packages (from xgboost) (1.16.3)\n",
      "Downloading xgboost-3.1.1-py3-none-win_amd64.whl (72.0 MB)\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/72.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/72.0 MB 1.6 MB/s eta 0:00:46\n",
      "   ---------------------------------------- 0.8/72.0 MB 1.5 MB/s eta 0:00:47\n",
      "    --------------------------------------- 1.3/72.0 MB 1.9 MB/s eta 0:00:38\n",
      "   - -------------------------------------- 2.1/72.0 MB 2.2 MB/s eta 0:00:33\n",
      "   - -------------------------------------- 2.6/72.0 MB 2.3 MB/s eta 0:00:31\n",
      "   - -------------------------------------- 3.1/72.0 MB 2.4 MB/s eta 0:00:30\n",
      "   -- ------------------------------------- 3.9/72.0 MB 2.5 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 4.5/72.0 MB 2.5 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 5.0/72.0 MB 2.6 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 5.8/72.0 MB 2.6 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 6.6/72.0 MB 2.7 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 7.1/72.0 MB 2.7 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 7.9/72.0 MB 2.8 MB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 8.4/72.0 MB 2.8 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 9.2/72.0 MB 2.9 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 9.7/72.0 MB 2.9 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 10.5/72.0 MB 2.9 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 11.3/72.0 MB 2.9 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 12.1/72.0 MB 3.0 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 12.6/72.0 MB 3.0 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 13.4/72.0 MB 3.0 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 14.2/72.0 MB 3.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 14.7/72.0 MB 3.0 MB/s eta 0:00:19\n",
      "   -------- ------------------------------- 15.5/72.0 MB 3.1 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 16.3/72.0 MB 3.1 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 17.0/72.0 MB 3.1 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 17.6/72.0 MB 3.1 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 18.4/72.0 MB 3.1 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 19.1/72.0 MB 3.1 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 19.9/72.0 MB 3.1 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 20.4/72.0 MB 3.1 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 21.2/72.0 MB 3.1 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 22.0/72.0 MB 3.2 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 22.5/72.0 MB 3.2 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 23.3/72.0 MB 3.2 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 24.1/72.0 MB 3.2 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 24.9/72.0 MB 3.2 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 25.7/72.0 MB 3.2 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 26.5/72.0 MB 3.2 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 27.3/72.0 MB 3.2 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 28.3/72.0 MB 3.3 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 29.1/72.0 MB 3.3 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 29.6/72.0 MB 3.3 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 30.7/72.0 MB 3.3 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 31.5/72.0 MB 3.3 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 32.5/72.0 MB 3.3 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 33.3/72.0 MB 3.4 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 34.1/72.0 MB 3.4 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 35.1/72.0 MB 3.4 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 36.2/72.0 MB 3.4 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 37.0/72.0 MB 3.5 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 38.0/72.0 MB 3.5 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 39.1/72.0 MB 3.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 40.1/72.0 MB 3.5 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 41.4/72.0 MB 3.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 42.5/72.0 MB 3.6 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 43.8/72.0 MB 3.7 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 45.1/72.0 MB 3.7 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 46.4/72.0 MB 3.8 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 48.0/72.0 MB 3.8 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 49.3/72.0 MB 3.8 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 50.9/72.0 MB 3.9 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 52.2/72.0 MB 3.9 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 53.7/72.0 MB 4.0 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 55.6/72.0 MB 4.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 57.4/72.0 MB 4.1 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 59.2/72.0 MB 4.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 61.1/72.0 MB 4.3 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 63.2/72.0 MB 4.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 65.0/72.0 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 67.4/72.0 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 69.5/72.0 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  71.8/72.0 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 72.0/72.0 MB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\thhnp\\Thesis PM25 Prediction\\tfvenv\\Scripts\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1eae503-b96a-4a33-96fc-1227f1d3e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeCV, LogisticRegressionCV\n",
    "from sklearn.ensemble import StackingRegressor, StackingClassifier\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26c63f02-dc22-48cb-b5be-ee53f62981e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading and Cleaning Data...\n",
      "2. Generating Features...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_gru_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 143\u001b[39m\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# GRU for Regression (Peak Value)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m gru_reg = \u001b[43mcreate_gru_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_units\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_activation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlinear\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmse\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# GRU for Classification (Peak Time, 24 classes)\u001b[39;00m\n\u001b[32m    145\u001b[39m gru_class = create_gru_model(output_units=\u001b[32m24\u001b[39m, output_activation=\u001b[33m'\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m'\u001b[39m, loss_func=\u001b[33m'\u001b[39m\u001b[33msparse_categorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mcreate_gru_model\u001b[39m\u001b[34m(output_units, output_activation, loss_func)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_gru_model\u001b[39m(output_units, output_activation, loss_func):\n\u001b[32m    132\u001b[39m     model = Sequential([\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m         GRU(units=\u001b[32m64\u001b[39m, return_sequences=\u001b[38;5;28;01mTrue\u001b[39;00m, input_shape=(LOOKBACK_WINDOW, \u001b[43mX_gru_train_scaled\u001b[49m.shape[\u001b[32m1\u001b[39m])),\n\u001b[32m    134\u001b[39m         Dropout(\u001b[32m0.2\u001b[39m),\n\u001b[32m    135\u001b[39m         GRU(units=\u001b[32m32\u001b[39m, return_sequences=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m    136\u001b[39m         Dropout(\u001b[32m0.2\u001b[39m),\n\u001b[32m    137\u001b[39m         Dense(output_units, activation=output_activation)\n\u001b[32m    138\u001b[39m     ])\n\u001b[32m    139\u001b[39m     model.compile(optimizer=Adam(learning_rate=\u001b[32m0.001\u001b[39m), loss=loss_func)\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[31mNameError\u001b[39m: name 'X_gru_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "FILE_PATH = './datasets/ratnapark_pm25_after_imputation.csv'\n",
    "TARGET_COL = 'PM2.5'\n",
    "# Prediction window (24 hours ahead, from t+1 to t+24)\n",
    "PRED_WINDOW = 24 \n",
    "# Look-back window for lagged and sequence features\n",
    "LOOKBACK_WINDOW = 48 \n",
    "METEO_FEATURES = ['PS', 'WS2M', 'RH2M', 'T2M'] # Key meteorological features\n",
    "\n",
    "# --- 2. DATA LOADING AND TIME INDEXING ---\n",
    "print(\"1. Loading and Cleaning Data...\")\n",
    "df = pd.read_csv(FILE_PATH, index_col=0)\n",
    "# df['Datetime'] = pd.to_datetime(df[['YEAR', 'MO', 'DY', 'HR']])\n",
    "df['date'] = pd.to_datetime(df.rename(columns={'YEAR': 'year', 'MO': 'month', 'DY': 'day'})[['year', 'month', 'day']])\n",
    "df = df.set_index('date').sort_index()\n",
    "# Select relevant features\n",
    "features_to_use = [TARGET_COL] + METEO_FEATURES\n",
    "df = df[features_to_use]\n",
    "\n",
    "# --- 3. TARGET FEATURE ENGINEERING (Peak Value & Peak Time) ---\n",
    "\n",
    "# Shift the PM2.5 series to align the target:\n",
    "# At time t, we look for the max value and its time in the window [t+1, t+24]\n",
    "df['Peak_Value_Target'] = np.nan\n",
    "df['Peak_Time_Target'] = np.nan\n",
    "\n",
    "for i in range(len(df) - PRED_WINDOW):\n",
    "    # Define the 24-hour window: t+1 to t+24\n",
    "    future_window = df[TARGET_COL].iloc[i+1 : i+1+PRED_WINDOW]\n",
    "    \n",
    "    # Target 1 (Regression): Peak PM2.5 value\n",
    "    peak_value = future_window.max()\n",
    "    df.loc[df.index[i], 'Peak_Value_Target'] = peak_value\n",
    "    \n",
    "    # Target 2 (Classification): Hour of occurrence (0 to 23)\n",
    "    # Get the index (timestamp) where the peak occurred\n",
    "    peak_timestamp = future_window.idxmax()\n",
    "    # Extract the hour from the timestamp (0-23)\n",
    "    peak_hour = peak_timestamp.hour\n",
    "    df.loc[df.index[i], 'Peak_Time_Target'] = peak_hour\n",
    "\n",
    "# Drop the last 24 rows as they do not have a full prediction window\n",
    "df = df.dropna()\n",
    "\n",
    "# --- 4. FEATURE ENGINEERING (Lags, Cyclical, Seasonality) ---\n",
    "\n",
    "print(\"2. Generating Features...\")\n",
    "\n",
    "# A. Lagged Features (Flat Features for XGBoost/RF)\n",
    "for col in features_to_use:\n",
    "    for lag in [1, 6, 12, 24, 48]: # Common lags to capture hourly/daily patterns\n",
    "        df[f'{col}_lag{lag}'] = df[col].shift(lag)\n",
    "\n",
    "# B. Temporal/Cyclical Features\n",
    "df['HR'] = df.index.hour\n",
    "df['MO'] = df.index.month\n",
    "# Cyclical encoding for Hour (optional, but good practice)\n",
    "df['HR_sin'] = np.sin(2 * np.pi * df['HR'] / 24)\n",
    "df['HR_cos'] = np.cos(2 * np.pi * df['HR'] / 24)\n",
    "df = df.drop(columns=['HR'])\n",
    "\n",
    "# C. Seasonality (Categorical Feature)\n",
    "def get_season(month):\n",
    "    if month in [1, 2]: return 'Winter'\n",
    "    elif month in [3, 4, 5]: return 'Pre_Monsoon'\n",
    "    elif month in [6, 7, 8]: return 'Monsoon'\n",
    "    else: return 'Post_Monsoon' # 9, 10, 11, 12\n",
    "\n",
    "df['Season'] = df['MO'].apply(get_season)\n",
    "df = pd.get_dummies(df, columns=['Season'], drop_first=True)\n",
    "df = df.drop(columns=['MO'])\n",
    "\n",
    "# Drop rows with NaN values created by lagging\n",
    "df = df.dropna()\n",
    "\n",
    "# Final Feature Sets\n",
    "Y_peak_value = df['Peak_Value_Target'].values\n",
    "Y_peak_time = df['Peak_Time_Target'].astype(int).values # Classification labels\n",
    "X_flat = df.drop(columns=['Peak_Value_Target', 'Peak_Time_Target']).copy()\n",
    "flat_feature_names = X_flat.columns\n",
    "\n",
    "# --- 5. SCALING AND SPLITTING ---\n",
    "\n",
    "# Use 80% for training/validation and 20% for testing\n",
    "split_point = int(len(X_flat) * 0.8)\n",
    "X_train_flat, X_test_flat = X_flat.iloc[:split_point], X_flat.iloc[split_point:]\n",
    "Y_train_val, Y_test = Y_peak_value[:split_point], Y_peak_value[split_point:]\n",
    "Y_train_class, Y_test_class = Y_peak_time[:split_point], Y_peak_time[split_point:]\n",
    "\n",
    "# Scaler (Fit only on training data)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_flat)\n",
    "X_test_scaled = scaler.transform(X_test_flat)\n",
    "\n",
    "# Reshape data for GRU (Time Series Input)\n",
    "# GRU features are a subset of flat features, shaped as (samples, timesteps, features)\n",
    "# We will use only PM2.5 and meteorological data for sequence input\n",
    "gru_features = [col for col in X_flat.columns if '_lag' not in col]\n",
    "X_gru_flat_train = X_train_flat[gru_features].values\n",
    "X_gru_flat_test = X_test_flat[gru_features].values\n",
    "gru_scaler = MinMaxScaler()\n",
    "X_gru_train_scaled = gru_scaler.fit_transform(X_gru_flat_train)\n",
    "X_gru_test_scaled = gru_scaler.transform(X_gru_flat_test)\n",
    "\n",
    "def create_sequences(X, y_reg, y_class, time_steps):\n",
    "    X_seq, Y_reg, Y_class = [], [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        # Sequence input: t-LOOKBACK_WINDOW to t-1\n",
    "        X_seq.append(X[i:i + time_steps])\n",
    "        # Target: value at time t (which is the peak of t+1 to t+24)\n",
    "        Y_reg.append(y_reg[i + time_steps])\n",
    "        Y_class.append(y_class[i + time_steps])\n",
    "    return np.array(X_seq), np.array(Y_reg), np.array(Y_class)\n",
    "\n",
    "# The GRU sequence creation needs to be offset due to the initial target dropping.\n",
    "# For simplicity, we create sequences based on the already cleaned and lagged data,\n",
    "# adjusting the index appropriately.\n",
    "X_seq_train, Y_reg_train, Y_class_train = create_sequences(\n",
    "    X_gru_train_scaled, Y_train_val[LOOKBACK_WINDOW:], Y_train_class[LOOKBACK_WINDOW:], LOOKBACK_WINDOW\n",
    ")\n",
    "X_seq_test, Y_reg_test, Y_class_test = create_sequences(\n",
    "    X_gru_test_scaled, Y_test[LOOKBACK_WINDOW:], Y_test_class[LOOKBACK_WINDOW:], LOOKBACK_WINDOW\n",
    ")\n",
    "\n",
    "# Align flat features for the subset that has matching sequence data\n",
    "X_train_aligned = X_train_scaled[LOOKBACK_WINDOW:]\n",
    "X_test_aligned = X_test_scaled[LOOKBACK_WINDOW:]\n",
    "\n",
    "# --- 6. GRU MODEL DEFINITION ---\n",
    "\n",
    "def create_gru_model(output_units, output_activation, loss_func):\n",
    "    model = Sequential([\n",
    "        GRU(units=64, return_sequences=True, input_shape=(LOOKBACK_WINDOW, X_gru_train_scaled.shape[1])),\n",
    "        Dropout(0.2),\n",
    "        GRU(units=32, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(output_units, activation=output_activation)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=loss_func)\n",
    "    return model\n",
    "\n",
    "# GRU for Regression (Peak Value)\n",
    "gru_reg = create_gru_model(output_units=1, output_activation='linear', loss_func='mse')\n",
    "# GRU for Classification (Peak Time, 24 classes)\n",
    "gru_class = create_gru_model(output_units=24, output_activation='softmax', loss_func='sparse_categorical_crossentropy')\n",
    "\n",
    "# Train GRU models (using a subset of training data for simplicity)\n",
    "print(\"\\n3. Training GRU Models...\")\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Note: In a real thesis, you would train these for many epochs.\n",
    "gru_reg.fit(X_seq_train, Y_reg_train, epochs=10, batch_size=32, validation_split=0.1, callbacks=[es], verbose=0)\n",
    "gru_class.fit(X_seq_train, Y_class_train, epochs=10, batch_size=32, validation_split=0.1, callbacks=[es], verbose=0)\n",
    "\n",
    "# Create GRU prediction wrappers for StackingRegressor/Classifier\n",
    "# These classes allow Keras models to be used as estimators in Scikit-learn's Stacking framework\n",
    "\n",
    "class GRU_Wrapper_Reg:\n",
    "    def __init__(self, model): self.model = model\n",
    "    def fit(self, X, y): self.model.fit(X_seq_train, y, epochs=1, batch_size=32, verbose=0) # Re-train on full data\n",
    "    def predict(self, X): return self.model.predict(X_seq_test).flatten()\n",
    "    def get_params(self, deep=True): return {}\n",
    "\n",
    "class GRU_Wrapper_Class:\n",
    "    def __init__(self, model): self.model = model\n",
    "    def fit(self, X, y): self.model.fit(X_seq_train, y, epochs=1, batch_size=32, verbose=0)\n",
    "    def predict(self, X): return np.argmax(self.model.predict(X_seq_test), axis=1) # Predicts hard class\n",
    "    def predict_proba(self, X): return self.model.predict(X_seq_test) # Predicts class probabilities\n",
    "    def get_params(self, deep=True): return {}\n",
    "    def classes_(self): return np.arange(24) # Needed for StackingClassifier\n",
    "\n",
    "gru_reg_base = GRU_Wrapper_Reg(gru_reg)\n",
    "gru_class_base = GRU_Wrapper_Class(gru_class)\n",
    "\n",
    "\n",
    "# --- 7. STACKING ENSEMBLE DEFINITION AND TRAINING ---\n",
    "\n",
    "# Base Models (Level 0) - For the stacking pipeline, we use the aligned flat features\n",
    "# We include the GRU predictions as an *additional feature* for the meta-learner, not as a direct base estimator.\n",
    "# For simplicity and robust integration, we define the base estimators using the tree-based models,\n",
    "# and then manually integrate the GRU predictions for the Level 1 Meta-Learner.\n",
    "\n",
    "base_estimators_reg = [\n",
    "    ('xgb_reg', XGBRegressor(n_estimators=100, random_state=42)),\n",
    "    ('rf_reg', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "]\n",
    "base_estimators_class = [\n",
    "    ('xgb_class', XGBClassifier(n_estimators=100, use_label_encoder=False, eval_metric='mlogloss', random_state=42)),\n",
    "    ('rf_class', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "# Meta-Learner (Level 1)\n",
    "meta_reg = RidgeCV()\n",
    "meta_class = LogisticRegressionCV(max_iter=5000, multi_class='multinomial')\n",
    "\n",
    "print(\"4. Training Stacking Models...\")\n",
    "\n",
    "# 7.1. Peak Value Prediction (Regression) Stacking Pipeline\n",
    "stack_reg = StackingRegressor(\n",
    "    estimators=base_estimators_reg,\n",
    "    final_estimator=meta_reg,\n",
    "    cv=5, # 5-fold cross-validation for generating Level 0 predictions\n",
    "    n_jobs=-1\n",
    ")\n",
    "stack_reg.fit(X_train_aligned, Y_reg_train)\n",
    "y_pred_reg = stack_reg.predict(X_test_aligned)\n",
    "\n",
    "# 7.2. Peak Time Prediction (Classification) Stacking Pipeline\n",
    "stack_class = StackingClassifier(\n",
    "    estimators=base_estimators_class,\n",
    "    final_estimator=meta_class,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "stack_class.fit(X_train_aligned, Y_class_train)\n",
    "y_pred_class = stack_class.predict(X_test_aligned)\n",
    "\n",
    "\n",
    "# --- 8. EVALUATION ---\n",
    "\n",
    "print(\"\\n--- 5. MODEL EVALUATION ---\")\n",
    "\n",
    "# Evaluate Regression (Peak Value)\n",
    "r2_reg = r2_score(Y_reg_test, y_pred_reg)\n",
    "rmse_reg = mean_squared_error(Y_reg_test, y_pred_reg, squared=False)\n",
    "print(f\"\\n[PEAK VALUE REGRESSION METRICS]\")\n",
    "print(f\"R-squared (R2): {r2_reg:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_reg:.2f}\")\n",
    "\n",
    "# Evaluate Classification (Peak Time)\n",
    "accuracy_class = accuracy_score(Y_class_test, y_pred_class)\n",
    "f1_macro_class = f1_score(Y_class_test, y_pred_class, average='macro')\n",
    "print(f\"\\n[PEAK TIME CLASSIFICATION METRICS]\")\n",
    "print(f\"Classification Accuracy: {accuracy_class:.4f}\")\n",
    "print(f\"F1-Score (Macro): {f1_macro_class:.4f}\")\n",
    "\n",
    "# Custom Time Error Metric (Important for time prediction)\n",
    "# Calculates the minimum hour difference (e.g., 23 vs 0 is 1 hour difference)\n",
    "def circular_time_error(y_true, y_pred, max_time=24):\n",
    "    error = np.abs(y_true - y_pred)\n",
    "    # The error should be the minimum distance around the clock (0 to 23)\n",
    "    circular_error = np.minimum(error, max_time - error)\n",
    "    return np.mean(circular_error)\n",
    "\n",
    "time_error = circular_time_error(Y_class_test, y_pred_class)\n",
    "print(f\"Mean Circular Time Error (Hours): {time_error:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43aca68-5084-45f2-b633-edc5c052d44d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TensorFlow)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
